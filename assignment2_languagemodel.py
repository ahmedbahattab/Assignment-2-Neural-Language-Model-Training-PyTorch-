# -*- coding: utf-8 -*-
"""Assignment2_LanguageModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E-p7A_TwAyq5zbYQBbd5qneOwySZx0I9
"""

# Step 1: Environment setup
!pip install --quiet torch torchvision torchaudio matplotlib

import sys, random, os, math
import numpy as np
import torch

# reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Python:", sys.version.splitlines()[0])
print("PyTorch:", torch.__version__)
print("Device:", device)
if device.type == "cuda":
    try:
        print("GPU:", torch.cuda.get_device_name(0),
              "Memory(GB):", round(torch.cuda.get_device_properties(0).total_memory/1024**3,2))
    except:
        pass

# Step 2: Upload and clean the Pride & Prejudice text
from google.colab import files
import re

print("Upload the provided Pride_and_Prejudice txt file (Project Gutenberg) ...")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print("Uploaded:", filename)

with open(filename, "r", encoding="utf-8", errors="ignore") as f:
    raw = f.read()

lower = raw.lower()

# find story start using canonical phrase
start_phrase = "it is a truth universally acknowledged"
s = lower.find(start_phrase)
if s == -1:
    raise RuntimeError("Could not find story start phrase.")

# find story end using candidate phrases
end_candidates = ["darcy as to be sensible of your kindness", "darcy, as to be sensible of your kindness", "the end"]
e = -1
for cand in end_candidates:
    pos = lower.rfind(cand)
    if pos != -1:
        e = pos + len(cand)
        break
if e == -1:
    e = len(raw)

clean_text = raw[s:e].strip().lower()

# save cleaned file (deliverable)
os.makedirs("data", exist_ok=True)
clean_fn = "data/Pride_and_Prejudice_CLEANED.txt"
with open(clean_fn, "w", encoding="utf-8") as f:
    f.write(clean_text)

print("Cleaned text saved to", clean_fn)
print("Preview:\n")
print(clean_text[:500])
print("\nLength:", len(clean_text))

# Step 3: Build character vocabulary and encode
chars = sorted(list(set(clean_text)))
vocab_size = len(chars)
print("Unique characters:", vocab_size)
print("Sample chars:", chars[:30], " ...")

char_to_int = {c:i for i,c in enumerate(chars)}
int_to_char = {i:c for c,i in char_to_int.items()}

encoded_text = [char_to_int[c] for c in clean_text]
print("Encoded text length:", len(encoded_text))
print("Example encoded (first 20):", encoded_text[:20])

# Step 4: Create sequences and DataLoader
import torch
from torch.utils.data import Dataset, DataLoader

SEQ_LEN = 100
X, Y = [], []
for i in range(len(encoded_text) - SEQ_LEN):
    X.append(encoded_text[i:i+SEQ_LEN])
    Y.append(encoded_text[i+SEQ_LEN])

print("Total sequences:", len(X))

split = int(0.9 * len(X))
X_train, Y_train = X[:split], Y[:split]
X_val, Y_val = X[split:], Y[split:]

print("Train sequences:", len(X_train))
print("Val sequences:", len(X_val))

class CharDataset(Dataset):
    def __init__(self, X, Y):
        self.X = X; self.Y = Y
    def __len__(self): return len(self.X)
    def __getitem__(self, idx):
        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.Y[idx], dtype=torch.long)

BATCH_SIZE = 128
train_loader = DataLoader(CharDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(CharDataset(X_val, Y_val), batch_size=BATCH_SIZE, shuffle=False)

xb, yb = next(iter(train_loader))
print("Batch shapes:", xb.shape, yb.shape)

# Step 5: A simple 1-layer LSTM model
import torch.nn as nn

class SimpleLSTM(nn.Module):
    def __init__(self, vocab_size, embed_size=64, hidden_size=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)
    def forward(self, x):
        x = self.embedding(x)              # (batch, seq, emb)
        out, _ = self.lstm(x)              # (batch, seq, hidden)
        out = out[:, -1, :]                # (batch, hidden)
        logits = self.fc(out)              # (batch, vocab)
        return logits

# instantiate best-fit model
best_model = SimpleLSTM(vocab_size, embed_size=64, hidden_size=128).to(device)
print(best_model)

# Step 6: Simple training + validation function
import torch.optim as optim
import math
criterion = nn.CrossEntropyLoss()

def perplexity(loss):
    return math.exp(loss)

def train_model(model, train_loader, val_loader, epochs=5, lr=0.001):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    train_losses, val_losses = [], []
    for epoch in range(1, epochs+1):
        # train
        model.train()
        total_train = 0.0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            logits = model(xb)
            loss = criterion(logits, yb)
            loss.backward()
            optimizer.step()
            total_train += loss.item()
        avg_train = total_train / len(train_loader)
        train_losses.append(avg_train)

        # val
        model.eval()
        total_val = 0.0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                logits = model(xb)
                loss = criterion(logits, yb)
                total_val += loss.item()
        avg_val = total_val / len(val_loader)
        val_losses.append(avg_val)

        print(f"Epoch {epoch}/{epochs} | Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f} | Val PPL: {perplexity(avg_val):.2f}")
    return train_losses, val_losses

# Step 7: Train best-fit model
best_train_losses, best_val_losses = train_model(
    best_model,
    train_loader,
    val_loader,
    epochs=5,
    lr=0.001
)

# Plot and save best-fit loss curve
import matplotlib.pyplot as plt
os.makedirs("plots", exist_ok=True)

plt.figure(figsize=(6,4))
plt.plot(best_train_losses, label='Train Loss')
plt.plot(best_val_losses, label='Val Loss')
plt.title('Best-Fit Model Loss (Simple LSTM)')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)
plt.savefig("plots/bestfit_loss.png", dpi=150, bbox_inches='tight')
plt.show()
print("Saved: plots/bestfit_loss.png")

# Step 8: Underfitting experiment
under_model = SimpleLSTM(vocab_size, embed_size=16, hidden_size=32).to(device)
print("Underfitting model:", under_model)
under_train_losses, under_val_losses = train_model(under_model, train_loader, val_loader, epochs=3, lr=0.001)
# plot and save
plt.figure(figsize=(6,4))
plt.plot(under_train_losses, label='Train Loss')
plt.plot(under_val_losses, label='Val Loss')
plt.title('Underfitting Model Loss')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)
plt.savefig("plots/underfit_loss.png", dpi=150, bbox_inches='tight')
plt.show()
print("Saved: plots/underfit_loss.png")

# Step 9: Overfitting experiment
# Make small subset (1000 sequences)
small_X = X_train[:1000]; small_Y = Y_train[:1000]
small_loader = DataLoader(CharDataset(small_X, small_Y), batch_size=128, shuffle=True)
print("Small subset size:", len(small_X))

over_model = SimpleLSTM(vocab_size, embed_size=128, hidden_size=256).to(device)
print("Overfitting model:", over_model)

over_train_losses, over_val_losses = train_model(over_model, small_loader, val_loader, epochs=7, lr=0.01)

# plot and save
plt.figure(figsize=(6,4))
plt.plot(over_train_losses, label='Train Loss')
plt.plot(over_val_losses, label='Val Loss')
plt.title('Overfitting Model Loss')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)
plt.savefig("plots/overfit_loss.png", dpi=150, bbox_inches='tight')
plt.show()
print("Saved: plots/overfit_loss.png")

# Step 10: Save trained models (deliverables)
os.makedirs("models", exist_ok=True)
torch.save(best_model.state_dict(), "models/bestfit_model.pt")
torch.save(under_model.state_dict(), "models/underfit_model.pt")
torch.save(over_model.state_dict(), "models/overfit_model.pt")
print("Saved models to models/*.pt")

# Step 11: Print a small summary table with final metrics
def final_ppl(val_losses):
    return math.exp(val_losses[-1])

print("FINAL METRICS:")
print("Best-fit Val Loss:", best_val_losses[-1], "PPL:", final_ppl(best_val_losses))
print("Underfit Val Loss:", under_val_losses[-1], "PPL:", final_ppl(under_val_losses))
print("Overfit Val Loss:", over_val_losses[-1], "PPL:", final_ppl(over_val_losses))